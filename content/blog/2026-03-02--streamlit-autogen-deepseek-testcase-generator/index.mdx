---
title: 用 Streamlit + AutoGen + DeepSeek 落地智能测试用例生成器
slug: streamlit-autogen-deepseek-testcase-generator
description: 基于一篇实战案例，拆解如何用 Streamlit、AutoGen 与 DeepSeek 构建可落地的测试用例生成系统，并补充工程化实现要点。
date: 2026-03-02
lastUpdated: 2026-03-02
tags:
  - Tool
  - Tutorial
---

:::note[引用说明]
本文基于魏文曼的文章整理与延展：[当测试遇见AI：用 Streamlit+AutoGen+Deepseek 构建智能用例生成器](https://mp.weixin.qq.com/s/fRuyPH3tyInDgVGTk4pOAg)。
:::

很多团队在测试环节的真实瓶颈，不是不会测，而是「重复劳动太多」：需求一改，用例就要大面积返工；多人协作时，格式、编号和覆盖范围又容易失控。原文给出的方案是 `Streamlit + AutoGen + DeepSeek`，这套组合的价值不在于“替代测试”，而在于把测试工程师从机械编写中解放出来。

本文按「为什么值得做 -> 方案怎么搭 -> 工程上怎么落地」展开，并补充我认为在生产环境必须加上的控制点。

## 先看结论：这套组合解决的是什么问题

推荐把它定义为“测试设计加速器”，而不是“自动测试平台”。它最适合三类任务：

1. 把自然语言需求快速转换成结构化测试用例草案。
2. 自动补齐边界条件，降低漏测概率。
3. 保证输出格式统一，减少评审中的格式争议。

不推荐直接把模型输出当最终发布版本。正确做法是：AI 负责初稿和补全，人类负责风险判断与验收。

## 架构拆解：三层职责要清晰

原文的分层是合理的，可以直接沿用：

- 交互层：`Streamlit`
- 编排层：`AutoGen`
- 生成层：`DeepSeek`

### 1. Streamlit：用最小成本把交互跑起来

Streamlit 在这个场景的优势不是“界面好看”，而是“能快速做出可用工作台”：需求输入、生成参数、结果预览、下载导出，这些都可以在同一页面闭环。

对团队来说，这一步能把“命令行脚本”升级为“人人可用工具”，推进成本很低。

### 2. AutoGen：把单次调用升级为可控流程

如果只调用一次模型，系统很快会遇到两个问题：输出不稳定、调试困难。AutoGen 的价值是把流程显式化：

- 代理职责分工（需求解析、用例生成、格式校验）。
- 对话上下文可追溯。
- 失败路径可定义（重试、回退、人工确认）。

这一步决定了系统是“演示 Demo”还是“可维护工具”。

### 3. DeepSeek：负责语义理解与结构化生成

原文强调了 DeepSeek 在中文语境和结构化输出上的效果。实践中真正关键的不是模型名称，而是两件事：

- Prompt 要把约束写成“硬规则”（数量、ID 唯一性、字段完整性）。
- 输出要经过程序级验证，不要只靠模型自检。

## 工程落地的关键：约束外化，不信任模型的“自觉”

原文里有一个值得保留的做法：生成后再做二次校验。这个思路非常重要。下面给一个精简版实现：

```python
import re
from collections import Counter

def validate_cases(markdown: str, expected_count: int) -> list[str]:
    warnings: list[str] = []

    ids = re.findall(r'(?:用例ID|test case ID)[:：]\s*([A-Z_0-9]+)', markdown, re.IGNORECASE)

    if len(ids) != expected_count:
        warnings.append(f'数量不匹配：期望 {expected_count}，实际 {len(ids)}')

    duplicated = [k for k, v in Counter(ids).items() if v > 1]
    if duplicated:
        warnings.append(f'存在重复 ID：{", ".join(duplicated)}')

    return warnings
```

这段代码不复杂，但它把“质量门”从主观判断变成了可执行规则。我的建议是继续加两类校验：

1. 覆盖校验：关键业务路径是否都出现。
2. 边界校验：边界值与异常输入是否齐全。

## 从 Demo 到生产，还需要补哪几块

原文已经提到历史记录、评审辅助、diff 对比等方向。若要投入团队使用，我建议优先补这四项：

### 1. 可追踪性

给每次生成分配 `request_id`，保存需求、prompt 版本、模型版本、输出结果和校验报告。出现问题时才能准确复盘。

### 2. 版本治理

把系统提示词（system prompt）和模板按版本管理，避免“同样输入，不同时间输出风格突变”。

### 3. 人工审核关口

在发布前增加“审核通过”状态，禁止未审核用例直接进入回归库。

### 4. 成本与延迟监控

记录每次生成耗时、token 用量、失败率。否则很难优化体验，也无法控制预算。

## 方案边界与 trade-off

推荐这条技术路线的原因：交付速度快、迭代成本低、对现有测试流程侵入小。

主要 trade-off 也很明确：

- 收益：能显著减少机械劳动，提高初稿产出速度。
- 代价：需要额外建设校验、审核、追踪与治理能力。

如果只做“输入需求 -> 输出用例”的最短链路，短期会很惊艳，长期会被稳定性和一致性反噬。

## 总结

`Streamlit + AutoGen + DeepSeek` 这套组合是一个高性价比起点，适合先在团队内部做“辅助生成 + 人工审核”的落地试点。工程上要坚持一个原则：模型负责生成，系统负责约束，人类负责决策。把这三者边界分清，智能用例生成器才能从“可演示”走向“可依赖”。
